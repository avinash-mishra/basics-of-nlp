{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Most of the NLP tasks can be specified into following 5 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Training a word vector generation model ( such as Word2Vec) or loading pretrained word vectors\n",
    "2. Creating an ID's matrix for our training set (We'll discuss this a bit later)\n",
    "3. RNN (with LSTM units) graph creation\n",
    "4. Training \n",
    "5. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list\nproposed\nintelligence\ngiving\nhotel\nfinally\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word vectors\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0.]\n[ 0.013441  0.23682  -0.16899   0.40951   0.63812   0.47709  -0.42852\n -0.55641  -0.364    -0.23938   0.13001  -0.063734 -0.39575  -0.48162\n  0.23291   0.090201 -0.13324   0.078639 -0.41634  -0.15428   0.10068\n  0.48891   0.31226  -0.1252   -0.037512 -1.5179    0.12612  -0.02442\n -0.042961 -0.28351   3.5416   -0.11956  -0.014533 -0.1499    0.21864\n -0.33412  -0.13872   0.31806   0.70358   0.44858  -0.080262  0.63003\n  0.32111  -0.46765   0.22786   0.36034  -0.37818  -0.56657   0.044691\n  0.30392 ]\n[ 1.5164e-01  3.0177e-01 -1.6763e-01  1.7684e-01  3.1719e-01  3.3973e-01\n -4.3478e-01 -3.1086e-01 -4.4999e-01 -2.9486e-01  1.6608e-01  1.1963e-01\n -4.1328e-01 -4.2353e-01  5.9868e-01  2.8825e-01 -1.1547e-01 -4.1848e-02\n -6.7989e-01 -2.5063e-01  1.8472e-01  8.6876e-02  4.6582e-01  1.5035e-02\n  4.3474e-02 -1.4671e+00 -3.0384e-01 -2.3441e-02  3.0589e-01 -2.1785e-01\n  3.7460e+00  4.2284e-03 -1.8436e-01 -4.6209e-01  9.8329e-02 -1.1907e-01\n  2.3919e-01  1.1610e-01  4.1705e-01  5.6763e-02 -6.3681e-05  6.8987e-02\n  8.7939e-02 -1.0285e-01 -1.3931e-01  2.2314e-01 -8.0803e-02 -3.5652e-01\n  1.6413e-02  1.0216e-01]\n[ 0.70853    0.57088   -0.4716     0.18048    0.54449    0.72603\n  0.18157   -0.52393    0.10381   -0.17566    0.078852  -0.36216\n -0.11829   -0.83336    0.11917   -0.16605    0.061555  -0.012719\n -0.56623    0.013616   0.22851   -0.14396   -0.067549  -0.38157\n -0.23698   -1.7037    -0.86692   -0.26704   -0.2589     0.1767\n  3.8676    -0.1613    -0.13273   -0.68881    0.18444    0.0052464\n -0.33874   -0.078956   0.24185    0.36576   -0.34727    0.28483\n  0.075693  -0.062178  -0.38988    0.22902   -0.21617   -0.22562\n -0.093918  -0.80375  ]\n[ 0.68047  -0.039263  0.30186  -0.17792   0.42962   0.032246 -0.41376\n  0.13228  -0.29847  -0.085253  0.17118   0.22419  -0.10046  -0.43653\n  0.33418   0.67846   0.057204 -0.34448  -0.42785  -0.43275   0.55963\n  0.10032   0.18677  -0.26854   0.037334 -2.0932    0.22171  -0.39868\n  0.20912  -0.55725   3.8826    0.47466  -0.95658  -0.37788   0.20869\n -0.32752   0.12751   0.088359  0.16351  -0.21634  -0.094375  0.018324\n  0.21048  -0.03088  -0.19722   0.082279 -0.09434  -0.073297 -0.064699\n -0.26044 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "words_list =  np.load('data/sentiment_analysis/wordsList.npy')\n",
    "print('Loaded the word list')\n",
    "words_list = words_list.tolist()  # Originally loaded as numpy array\n",
    "words_list = [word.decode('UTF-8') for word in words_list]  # Encode words as UTF-8\n",
    "for i in range(5):\n",
    "    print(words_list[i])\n",
    "word_vectors = np.load('data/sentiment_analysis/wordVectors.npy') \n",
    "print('Loaded the word vectors')\n",
    "for i in range(5):\n",
    "    print(word_vectors[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make sure everything has been loaded in correctly, we can look the dimentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n2\n"
     ]
    }
   ],
   "source": [
    "print(len(words_list))\n",
    "print(len(word_vectors.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also search our wordlist for a word like \"baseball\", and then access its corresponding vector through the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444\n[-1.9327    1.0421   -0.78515   0.91033   0.22711  -0.62158  -1.6493\n  0.07686  -0.5868    0.058831  0.35628   0.68916  -0.50598   0.70473\n  1.2664   -0.40031  -0.020687  0.80863  -0.90566  -0.074054 -0.87675\n -0.6291   -0.12685   0.11524  -0.55685  -1.6826   -0.26291   0.22632\n  0.713    -1.0828    2.1231    0.49869   0.066711 -0.48226  -0.17897\n  0.47699   0.16384   0.16537  -0.11506  -0.15962  -0.94926  -0.42833\n -0.59457   1.3566   -0.27506   0.19918  -0.36008   0.55667  -0.70315\n  0.17157 ]\n"
     ]
    }
   ],
   "source": [
    "baseball_index = words_list.index('baseball')\n",
    "print(baseball_index)\n",
    "print(word_vectors[baseball_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take an input sentence and then constructing its vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n[    41    804 201534   1005     15   7446      5  13767      0      0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "max_seq_length = 10  # Maximum length of sentence\n",
    "num_dimensions = 300  # Dimensions for each word vector\n",
    "first_sentence = np.zeros((max_seq_length), dtype='int32')\n",
    "first_sentence[0] = words_list.index('i')\n",
    "first_sentence[1] = words_list.index('thought')\n",
    "first_sentence[2] = words_list.index('the')\n",
    "first_sentence[3] = words_list.index('movie')\n",
    "first_sentence[4] = words_list.index('was')\n",
    "first_sentence[5] = words_list.index('incredible')\n",
    "first_sentence[6] = words_list.index('and')\n",
    "first_sentence[7] = words_list.index('inspiring')\n",
    "\n",
    "# first_sentence[8] and first_sentence[9] are going to be zero\n",
    "print(first_sentence.shape)\n",
    "print(first_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(tf.nn.embedding_lookup(word_vectors, first_sentence).eval().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 X 50 output  should contain the 50 dimentional word vectors for each of the 10 words in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
